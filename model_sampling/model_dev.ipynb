{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\brand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\brand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "import os\n",
    "import re\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x1bfa7bcd510>"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CWD = os.getcwd()\n",
    "dataset_engine = sqlalchemy.create_engine(f\"sqlite:///{CWD}/modeling_dataset.db\")\n",
    "dataset_engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>tbl_name</th>\n",
       "      <th>rootpage</th>\n",
       "      <th>sql</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>table</td>\n",
       "      <td>by</td>\n",
       "      <td>by</td>\n",
       "      <td>2</td>\n",
       "      <td>CREATE TABLE \"by\" (\\n\\t\"index\" BIGINT, \\n\\tlic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>table</td>\n",
       "      <td>by-sa</td>\n",
       "      <td>by-sa</td>\n",
       "      <td>28</td>\n",
       "      <td>CREATE TABLE \"by-sa\" (\\n\\t\"index\" BIGINT, \\n\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>table</td>\n",
       "      <td>by-nc</td>\n",
       "      <td>by-nc</td>\n",
       "      <td>410</td>\n",
       "      <td>CREATE TABLE \"by-nc\" (\\n\\t\"index\" BIGINT, \\n\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>table</td>\n",
       "      <td>by-nc-sa</td>\n",
       "      <td>by-nc-sa</td>\n",
       "      <td>436</td>\n",
       "      <td>CREATE TABLE \"by-nc-sa\" (\\n\\t\"index\" BIGINT, \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>table</td>\n",
       "      <td>by-nd</td>\n",
       "      <td>by-nd</td>\n",
       "      <td>1223</td>\n",
       "      <td>CREATE TABLE \"by-nd\" (\\n\\t\"index\" BIGINT, \\n\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>table</td>\n",
       "      <td>by-nc-nd</td>\n",
       "      <td>by-nc-nd</td>\n",
       "      <td>1590</td>\n",
       "      <td>CREATE TABLE \"by-nc-nd\" (\\n\\t\"index\" BIGINT, \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>table</td>\n",
       "      <td>publicdomain</td>\n",
       "      <td>publicdomain</td>\n",
       "      <td>803</td>\n",
       "      <td>CREATE TABLE publicdomain (\\n\\t\"index\" BIGINT,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type          name      tbl_name  rootpage  \\\n",
       "0  table            by            by         2   \n",
       "1  table         by-sa         by-sa        28   \n",
       "2  table         by-nc         by-nc       410   \n",
       "3  table      by-nc-sa      by-nc-sa       436   \n",
       "4  table         by-nd         by-nd      1223   \n",
       "5  table      by-nc-nd      by-nc-nd      1590   \n",
       "6  table  publicdomain  publicdomain       803   \n",
       "\n",
       "                                                 sql  \n",
       "0  CREATE TABLE \"by\" (\\n\\t\"index\" BIGINT, \\n\\tlic...  \n",
       "1  CREATE TABLE \"by-sa\" (\\n\\t\"index\" BIGINT, \\n\\t...  \n",
       "2  CREATE TABLE \"by-nc\" (\\n\\t\"index\" BIGINT, \\n\\t...  \n",
       "3  CREATE TABLE \"by-nc-sa\" (\\n\\t\"index\" BIGINT, \\...  \n",
       "4  CREATE TABLE \"by-nd\" (\\n\\t\"index\" BIGINT, \\n\\t...  \n",
       "5  CREATE TABLE \"by-nc-nd\" (\\n\\t\"index\" BIGINT, \\...  \n",
       "6  CREATE TABLE publicdomain (\\n\\t\"index\" BIGINT,...  "
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = pd.read_sql(\"SELECT * FROM sqlite_master WHERE type = 'table'\", dataset_engine)\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>license</th>\n",
       "      <th>url</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4835</th>\n",
       "      <td>licenses/by-nc-nd/2.5</td>\n",
       "      <td>https://pitt.libguides.com/copyright/licenses</td>\n",
       "      <td>Creative Commons, Copyleft, and Other Licenses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5590</th>\n",
       "      <td>publicdomain/mark/1.0</td>\n",
       "      <td>https://rightsstatements.org/vocab/1.0/</td>\n",
       "      <td>Rights Statements Menu Statements About Docume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4349</th>\n",
       "      <td>licenses/by-nd/2.5</td>\n",
       "      <td>https://podcasts.apple.com/us/podcast/entrepre...</td>\n",
       "      <td>‎Entrepreneurial Thought Leaders on Apple Podc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5159</th>\n",
       "      <td>licenses/by-nc-nd/2.0</td>\n",
       "      <td>https://www.livescience.com/youngest-age-give-...</td>\n",
       "      <td>What's the youngest age that a person can get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4646</th>\n",
       "      <td>licenses/by-nc-nd/2.0</td>\n",
       "      <td>https://www.americanbar.org/groups/human_right...</td>\n",
       "      <td>Request unsuccessful. Incapsula incident ID: 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    license  \\\n",
       "4835  licenses/by-nc-nd/2.5   \n",
       "5590  publicdomain/mark/1.0   \n",
       "4349     licenses/by-nd/2.5   \n",
       "5159  licenses/by-nc-nd/2.0   \n",
       "4646  licenses/by-nc-nd/2.0   \n",
       "\n",
       "                                                    url  \\\n",
       "4835      https://pitt.libguides.com/copyright/licenses   \n",
       "5590            https://rightsstatements.org/vocab/1.0/   \n",
       "4349  https://podcasts.apple.com/us/podcast/entrepre...   \n",
       "5159  https://www.livescience.com/youngest-age-give-...   \n",
       "4646  https://www.americanbar.org/groups/human_right...   \n",
       "\n",
       "                                               contents  \n",
       "4835  Creative Commons, Copyleft, and Other Licenses...  \n",
       "5590  Rights Statements Menu Statements About Docume...  \n",
       "4349  ‎Entrepreneurial Thought Leaders on Apple Podc...  \n",
       "5159  What's the youngest age that a person can get ...  \n",
       "4646  Request unsuccessful. Incapsula incident ID: 2...  "
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages_dataset = pd.concat(\n",
    "    [\n",
    "        pd.read_sql(f\"SELECT * FROM '{table_name}'\", dataset_engine)\n",
    "        for table_name in tables[\"name\"]\n",
    "    ]\n",
    ")\n",
    "webpages_dataset = webpages_dataset\\\n",
    "    .loc[webpages_dataset[\"contents\"] != \"\", :]\\\n",
    "    .reset_index()\\\n",
    "    .drop([\"index\", \"title\", \"level_0\"], axis=1)\n",
    "webpages_dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>license</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1844</td>\n",
       "      <td>1844</td>\n",
       "      <td>1844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1844</td>\n",
       "      <td>39</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>http://agroportal.lirmm.fr/ontologies/PCO</td>\n",
       "      <td>licenses/by/2.1</td>\n",
       "      <td>403 Forbidden 403 Forbidden nginx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              url          license  \\\n",
       "count                                        1844             1844   \n",
       "unique                                       1844               39   \n",
       "top     http://agroportal.lirmm.fr/ontologies/PCO  licenses/by/2.1   \n",
       "freq                                            1              130   \n",
       "\n",
       "                                 contents  \n",
       "count                                1844  \n",
       "unique                               1788  \n",
       "top     403 Forbidden 403 Forbidden nginx  \n",
       "freq                                   25  "
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages_dataset_deduplicate = webpages_dataset.groupby(\"url\").first()\\\n",
    "    .reset_index()\n",
    "webpages_dataset_deduplicate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>license</th>\n",
       "      <th>Tool Typing</th>\n",
       "      <th>General Typing</th>\n",
       "      <th>Version</th>\n",
       "      <th>by</th>\n",
       "      <th>sa</th>\n",
       "      <th>nc</th>\n",
       "      <th>nd</th>\n",
       "      <th>neither</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>licenses/by/1.0</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>licenses/by/2.0</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>licenses/by/2.1</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>2.1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>licenses/by/2.5</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>2.5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>licenses/by/3.0</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>licenses/by/4.0</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            license Tool Typing General Typing  Version    by     sa     nc  \\\n",
       "0   licenses/by/1.0    licenses             by      1.0  True  False  False   \n",
       "14  licenses/by/2.0    licenses             by      2.0  True  False  False   \n",
       "27  licenses/by/2.1    licenses             by      2.1  True  False  False   \n",
       "33  licenses/by/2.5    licenses             by      2.5  True  False  False   \n",
       "39  licenses/by/3.0    licenses             by      3.0  True  False  False   \n",
       "45  licenses/by/4.0    licenses             by      4.0  True  False  False   \n",
       "\n",
       "       nd  neither  \n",
       "0   False    False  \n",
       "14  False    False  \n",
       "27  False    False  \n",
       "33  False    False  \n",
       "39  False    False  \n",
       "45  False    False  "
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataset_sampling\n",
    "license_map = dataset_sampling.get_license_map()\n",
    "license_ser = pd.concat([v for v in license_map.values()])\n",
    "license_ser_splits_df = license_ser.str.split(\"/\", expand=True)\n",
    "license_ser_splits_df = license_ser_splits_df.rename(\n",
    "    columns = {\n",
    "        0: \"Tool Typing\",\n",
    "        1: \"General Typing\",\n",
    "        2: \"Version\",\n",
    "        3: \"Jurisdiction\"\n",
    "    }\n",
    ")\n",
    "license_ser_splits_df[\"General Typing\"] = license_ser_splits_df[\"General Typing\"].str.replace(\"mark|zero\", \"publicdomain\", regex=True)\n",
    "license_ser_splits_df[\"General Typing\"] = license_ser_splits_df[\"General Typing\"].str.replace(\"by-nd-nc\", \"by-nc-nd\", regex=True)\n",
    "license_ser_splits_df[\"Version\"] = license_ser_splits_df[\"Version\"].astype(float)\n",
    "license_one_hot_encoding = pd.DataFrame()\n",
    "license_one_hot_encoding[\"by\"] = license_ser_splits_df[\"General Typing\"].str.contains(\"by\")\n",
    "license_one_hot_encoding[\"sa\"] = license_ser_splits_df[\"General Typing\"].str.contains(\"sa\")\n",
    "license_one_hot_encoding[\"nc\"] = license_ser_splits_df[\"General Typing\"].str.contains(\"nc\")\n",
    "license_one_hot_encoding[\"nd\"] = license_ser_splits_df[\"General Typing\"].str.contains(\"nd\")\n",
    "license_not_six_type = license_ser_splits_df[\"General Typing\"].str.contains(\"by|sa|nc|nd\")\n",
    "license_one_hot_encoding[\"neither\"] = ~(license_not_six_type.fillna(False))\n",
    "license_df = pd.concat([license_ser, license_ser_splits_df, license_one_hot_encoding], axis = 1)\\\n",
    "    .rename(columns = {0: \"license\"})\n",
    "license_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>license</th>\n",
       "      <th>contents</th>\n",
       "      <th>Tool Typing</th>\n",
       "      <th>General Typing</th>\n",
       "      <th>Version</th>\n",
       "      <th>by</th>\n",
       "      <th>sa</th>\n",
       "      <th>nc</th>\n",
       "      <th>nd</th>\n",
       "      <th>neither</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>https://www.archdaily.com/892597/ad-classics-f...</td>\n",
       "      <td>licenses/by/2.0</td>\n",
       "      <td>Gallery of AD Classics: French Communist Party...</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>https://journals.tulane.edu/SL/about/submissions</td>\n",
       "      <td>licenses/by/3.0</td>\n",
       "      <td>Submissions\\n\\t\\t\\t\\t\\t\\t\\t| Second Line - An ...</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>https://www.google.com/help/legalnotices_maps/</td>\n",
       "      <td>licenses/by/1.0</td>\n",
       "      <td>Legal Notices for Google Maps/Google Earth and...</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>http://kolibri.teacherinabox.org.au/modules/en...</td>\n",
       "      <td>licenses/by-sa/2.1</td>\n",
       "      <td>Creative Commons — Attribution-ShareAlike 2.1 ...</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by-sa</td>\n",
       "      <td>2.1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>https://www.repository.cam.ac.uk/handle/1810/2...</td>\n",
       "      <td>publicdomain/zero/1.0</td>\n",
       "      <td>Reduced monocyte and macrophage TNFSF15/TL1A e...</td>\n",
       "      <td>publicdomain</td>\n",
       "      <td>publicdomain</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "1220  https://www.archdaily.com/892597/ad-classics-f...   \n",
       "278    https://journals.tulane.edu/SL/about/submissions   \n",
       "1425     https://www.google.com/help/legalnotices_maps/   \n",
       "407   http://kolibri.teacherinabox.org.au/modules/en...   \n",
       "92    https://www.repository.cam.ac.uk/handle/1810/2...   \n",
       "\n",
       "                    license  \\\n",
       "1220        licenses/by/2.0   \n",
       "278         licenses/by/3.0   \n",
       "1425        licenses/by/1.0   \n",
       "407      licenses/by-sa/2.1   \n",
       "92    publicdomain/zero/1.0   \n",
       "\n",
       "                                               contents   Tool Typing  \\\n",
       "1220  Gallery of AD Classics: French Communist Party...      licenses   \n",
       "278   Submissions\\n\\t\\t\\t\\t\\t\\t\\t| Second Line - An ...      licenses   \n",
       "1425  Legal Notices for Google Maps/Google Earth and...      licenses   \n",
       "407   Creative Commons — Attribution-ShareAlike 2.1 ...      licenses   \n",
       "92    Reduced monocyte and macrophage TNFSF15/TL1A e...  publicdomain   \n",
       "\n",
       "     General Typing  Version     by     sa     nc     nd  neither  \n",
       "1220             by      2.0   True  False  False  False    False  \n",
       "278              by      3.0   True  False  False  False    False  \n",
       "1425             by      1.0   True  False  False  False    False  \n",
       "407           by-sa      2.1   True   True  False  False    False  \n",
       "92     publicdomain      1.0  False  False  False  False     True  "
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages_dataset_deduplicate = webpages_dataset_deduplicate.merge(license_df, on = \"license\")\n",
    "webpages_dataset_deduplicate.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>license</th>\n",
       "      <th>contents</th>\n",
       "      <th>Tool Typing</th>\n",
       "      <th>Version</th>\n",
       "      <th>by</th>\n",
       "      <th>sa</th>\n",
       "      <th>nc</th>\n",
       "      <th>nd</th>\n",
       "      <th>neither</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>General Typing</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>720</td>\n",
       "      <td>720</td>\n",
       "      <td>720</td>\n",
       "      <td>720</td>\n",
       "      <td>720</td>\n",
       "      <td>720</td>\n",
       "      <td>720</td>\n",
       "      <td>720</td>\n",
       "      <td>720</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by-nc</th>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by-nc-nd</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by-nc-sa</th>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by-nd</th>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by-sa</th>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publicdomain</th>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                url  license  contents  Tool Typing  Version   by   sa   nc  \\\n",
       "General Typing                                                                \n",
       "by              720      720       720          720      720  720  720  720   \n",
       "by-nc           225      225       225          225      225  225  225  225   \n",
       "by-nc-nd        107      107       107          107      107  107  107  107   \n",
       "by-nc-sa        188      188       188          188      188  188  188  188   \n",
       "by-nd           117      117       117          117      117  117  117  117   \n",
       "by-sa           263      263       263          263      263  263  263  263   \n",
       "publicdomain    224      224       224          224      224  224  224  224   \n",
       "\n",
       "                 nd  neither  \n",
       "General Typing                \n",
       "by              720      720  \n",
       "by-nc           225      225  \n",
       "by-nc-nd        107      107  \n",
       "by-nc-sa        188      188  \n",
       "by-nd           117      117  \n",
       "by-sa           263      263  \n",
       "publicdomain    224      224  "
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages_dataset_deduplicate.groupby(\"General Typing\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "#webpages_dataset_deduplicate[\"contents\"] = webpages_dataset_deduplicate[\"contents\"].apply(lambda x: x[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_unicodes(ser):\n",
    "    return ser.map(lambda x: \" \".join([c for c in x if c in string.printable]))\n",
    "\n",
    "def has_unicodes(s, tolerance = 25):\n",
    "    return np.sum([c not in string.printable for c in s]) <= tolerance\n",
    "\n",
    "def not_well_decrypted(s, tolerance = 25):\n",
    "    words = re.split(r\"\\s+\", s)\n",
    "    return np.sum([len(c) == 1 for c in words]) <= tolerance\n",
    "\n",
    "def remove_unicodes_aggressive(df, field_name = \"contents\"):\n",
    "    df_remove_unicode = df.loc[df[field_name].apply(has_unicodes), :]\n",
    "    return df_remove_unicode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Less Aggressive pruning\n",
    "#webpages_dataset_deduplicate[\"parsed_contents\"] = remove_unicodes(webpages_dataset_deduplicate[\"contents\"])\n",
    "#webpages_dataset_deduplicate.loc[1149, [\"contents\", \"parsed_contents\"]]\n",
    "\n",
    "# More Aggressive pruning\n",
    "webpages_dataset_deduplicate = remove_unicodes_aggressive(webpages_dataset_deduplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_less_than_c_chars(s, tolerance = 2):\n",
    "    words = re.split(r\"\\s+\", s)\n",
    "    return \" \".join([c for c in words if len(c) > tolerance and \"obj\" not in c])\n",
    "\n",
    "def remove_more_than_c_chars(s, tolerance = 15):\n",
    "    words = re.split(r\"\\s+\", s)\n",
    "    return \" \".join([c for c in words if len(c) <= tolerance])\n",
    "\n",
    "def remove_non_english(s):\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    return \" \".join(w for w in re.split(r\"\\s+\", s) if w in words)\n",
    "\n",
    "def remove_web_urls(s):\n",
    "    return re.sub(r\"[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", \" \", s)\n",
    "\n",
    "def clear_stopwords(s):\n",
    "    return \" \".join([word for word in re.split(r\"\\s+\", s) if word.lower() not in nltk.corpus.stopwords.words('english')])\n",
    "\n",
    "def clear_stopwords_series(ser):\n",
    "    return ser.apply(clear_stopwords)\n",
    "\n",
    "def overall_cleaning(ser):\n",
    "    cleaned_ser = ser.str.lower()\n",
    "    cleaned_ser = cleaned_ser.apply(remove_web_urls)\n",
    "    cleaned_ser = cleaned_ser.str.replace(r'[^A-Za-z\\s]', ' ', regex = True)\n",
    "    cleaned_ser = cleaned_ser.str.replace(r\"\\s+\", \" \", regex = True)\n",
    "    cleaned_ser = cleaned_ser.apply(clear_stopwords)\n",
    "    cleaned_ser = cleaned_ser.apply(remove_less_than_c_chars)\n",
    "    cleaned_ser = cleaned_ser.apply(remove_non_english)\n",
    "    #cleaned_ser = cleaned_ser.apply(remove_more_than_c_chars)\n",
    "    return cleaned_ser\n",
    "\n",
    "webpages_dataset_deduplicate[\"cleaned_contents\"] = overall_cleaning(webpages_dataset_deduplicate[\"contents\"])\n",
    "webpages_dataset_deduplicate = webpages_dataset_deduplicate.loc[\n",
    "    webpages_dataset_deduplicate[\"cleaned_contents\"].str.len() >= 500, :\n",
    "]\n",
    "#webpages_dataset_deduplicate[\"parsed_cleaned_contents\"] = clear_stopwords_series(webpages_dataset_deduplicate[\"cleaned_contents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry content:\n",
      "least open creative commons university skip main content like explorer older works best modern latest chrome safari edge continue browser may see unexpected creative commons least open search guide search creative commons least open guide designed walk different comes license work make educated choi\n",
      "\n",
      "Entry content:\n",
      "history state agricultural college history state agricultural college rex repository search rex collection rex digital faculty works student works submit rex home state digital morse department special state university history view item disabled browser site may work without history state agricultur\n",
      "\n",
      "Entry content:\n",
      "fine toggle navigation learn meet staff wisdom pocket video collection consulting mineral photography symposium shop browse search new vault thumbnail corner bases order mineral sale sale connect contact login advanced search advanced search login collection shop selection large historic collection \n",
      "\n",
      "Entry content:\n",
      "roll omega regular language learning library skip main content advertisement search cart log search search book book international conference construction analysis construction analysis cite roll omega regular language learning library book book roll omega regular language learning library sun fang \n",
      "\n",
      "Entry content:\n",
      "sliced pancake soup taste home food overview lower upper food quality overview agriculture organic country animal welfare ama seal agricultural harmony nature food safety control food culture overview history cuisine customs coffee house legacy tradition food overview wine special coffee beer cheese\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in webpages_dataset_deduplicate[\"cleaned_contents\"].sample(5):\n",
    "    print(\n",
    "        f\"Entry content:\\n{row[:300]}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'creativecommons org licenses by sa  '"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_url(url):\n",
    "    return \" \".join(re.split(r\"[_/\\.-]\", re.sub(r\"\\d\", \"\", url)))\n",
    "tokenize_url(\"creativecommons.org/licenses/by-sa/4.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = webpages_dataset_deduplicate.copy()\n",
    "dataset['token_url'] = dataset[\"url\"].apply(tokenize_url)\n",
    "dataset[\"train_text\"] = dataset[\"token_url\"] + \" \" + dataset[\"cleaned_contents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features_tfidf(train, test, text_field = \"train_text\"):\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True, max_df=0.9, stop_words=\"english\")\n",
    "    tfidf_vectorizer.fit_transform(train[text_field].values)\n",
    "    train_vectorized = tfidf_vectorizer.transform(train[text_field].values)\n",
    "    test_vectorized = tfidf_vectorizer.transform(test[text_field].values)\n",
    "    return train_vectorized, test_vectorized, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    523\n",
       "1    163\n",
       "2     72\n",
       "3    134\n",
       "4     73\n",
       "5    168\n",
       "6    167\n",
       "Name: url, dtype: int64"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby(\"General Typing\")[[\"url\"]].count().reset_index()[\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dataset = pd.DataFrame()\n",
    "dataset_counts = dataset.groupby(\"General Typing\")[[\"url\"]].count().reset_index()[\"url\"]\n",
    "license_dict = {\n",
    "        \"by\": 0,\n",
    "        \"by-nc\": 1,\n",
    "        \"by-nc-nd\": 2,\n",
    "        \"by-nc-sa\": 3,\n",
    "        \"by-nd\": 4,\n",
    "        \"by-sa\": 5,\n",
    "        \"publicdomain\": 6\n",
    "}\n",
    "\n",
    "model_dataset[\"train_text\"], model_dataset[\"General Typing\"] = dataset[\"cleaned_contents\"], dataset[\"General Typing\"]\n",
    "model_dataset[\"General Typing\"].replace(\n",
    "    license_dict,\n",
    "    inplace = True\n",
    ")\n",
    "\n",
    "training_set, test_set = train_test_split(model_dataset, test_size = 0.2)\n",
    "Y_train = training_set[\"General Typing\"].values\n",
    "Y_test = test_set[\"General Typing\"].values\n",
    "X_train, X_test, model_vecter = extract_text_features_tfidf(\n",
    "    training_set, test_set\n",
    ")\n",
    "\n",
    "smote = SMOTE(\n",
    "    sampling_strategy={\n",
    "        k: int(0.85 * max(round(np.mean(dataset_counts.values)), dataset_counts.iloc[k]))\n",
    "        for k in range(1, 7)} #\n",
    ")\n",
    "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logreg = LogisticRegression(\n",
    "    verbose = 0,\n",
    "    max_iter = 500,\n",
    "    penalty = 'l2',\n",
    "    solver = \"liblinear\",\n",
    "    C = 0.5,\n",
    "    class_weight = \"balanced\"\n",
    ").fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8688644688644689 \n",
      " 0.9567765567765568 \n",
      " 0.9816849816849816 \n",
      "==============\n",
      " 0.49230769230769234 \n",
      " 0.6653846153846154 \n",
      " 0.7884615384615384\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    top_k_accuracy_score(Y_train, model_logreg.predict_proba(X_train), k = 1),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_train, model_logreg.predict_proba(X_train), k = 2),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_train, model_logreg.predict_proba(X_train), k = 3),\n",
    "    \"\\n==============\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_logreg.predict_proba(X_test), k = 1),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_logreg.predict_proba(X_test), k = 2),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_logreg.predict_proba(X_test), k = 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "model_svc = SVC(\n",
    "    C = 0.5,\n",
    "    verbose = 1,\n",
    "    probability = True,\n",
    "    kernel = \"poly\",\n",
    "    degree = 1\n",
    ").fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9091575091575091 \n",
      " 0.9772893772893773 \n",
      " 0.9882783882783883 \n",
      "==============\n",
      " 0.4653846153846154 \n",
      " 0.6615384615384615 \n",
      " 0.8038461538461539\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    top_k_accuracy_score(Y_train, model_svc.predict_proba(X_train), k = 1),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_train, model_svc.predict_proba(X_train), k = 2),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_train, model_svc.predict_proba(X_train), k = 3),\n",
    "    \"\\n==============\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_svc.predict_proba(X_test), k = 1),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_svc.predict_proba(X_test), k = 2),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_svc.predict_proba(X_test), k = 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2'\n",
    "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "TRAINING_RATIO = 0.7\n",
    "\n",
    "model_dataset_copy = dataset.loc[:, [\"cleaned_contents\", \"General Typing\"]]\n",
    "model_dataset_copy[\"General Typing\"].replace(\n",
    "    license_dict,\n",
    "    inplace = True\n",
    ")\n",
    "target = model_dataset_copy.pop(\"General Typing\")\n",
    "# dataset_tensor = tf.convert_to_tensor(model_dataset, dtype=np.string_)\n",
    "tf_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((model_dataset_copy, target))\n",
    "    .shuffle(50)\n",
    ")\n",
    "train_dataset = tf_dataset.take(int(TRAINING_RATIO * len(tf_dataset)))\n",
    "test_dataset = tf_dataset.skip(int(TRAINING_RATIO * len(tf_dataset)))\n",
    "val_dataset = test_dataset.skip(\n",
    "    int((1 - TRAINING_RATIO) * 0.5 * len(tf_dataset))\n",
    ")\n",
    "test_dataset = test_dataset.take(\n",
    "    int((1 - TRAINING_RATIO) * 0.5 * len(tf_dataset))\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='parsed_cleaned_contents')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.5)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 1e-7\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 102s 3s/step - loss: 2.6373e-07 - accuracy: 0.4026 - val_loss: 3.4668e-07 - val_accuracy: 0.0153\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 98s 3s/step - loss: 2.6373e-07 - accuracy: 0.4004 - val_loss: 3.4790e-07 - val_accuracy: 0.0153\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 96s 3s/step - loss: 2.6438e-07 - accuracy: 0.3938 - val_loss: 3.4668e-07 - val_accuracy: 0.0204\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 97s 3s/step - loss: 2.6465e-07 - accuracy: 0.4147 - val_loss: 3.4486e-07 - val_accuracy: 0.0255\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 96s 3s/step - loss: 2.6465e-07 - accuracy: 0.3861 - val_loss: 3.3330e-07 - val_accuracy: 0.0306\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 94s 3s/step - loss: 2.6268e-07 - accuracy: 0.3938 - val_loss: 3.3817e-07 - val_accuracy: 0.0255\n",
      "Epoch 7/10\n",
      " 1/29 [>.............................] - ETA: 1:31 - loss: 7.1526e-07 - accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\UCBF22\\DSD Fall 2022\\model_sampling\\model_dev.ipynb Cell 38\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m classifier_model \u001b[39m=\u001b[39m build_classifier_model()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m classifier_model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m history \u001b[39m=\u001b[39m classifier_model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train_dataset,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1385\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2957\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1854\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "classifier_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = classifier_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd98d7e735064cd82a69955e27f6d0d35e8c81af503a9f702e870f7c3b7bf96a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
