{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\brand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\brand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "import os\n",
    "import re\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x1b651a12440>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CWD = os.getcwd()\n",
    "dataset_engine = sqlalchemy.create_engine(f\"sqlite:///{CWD}/modeling_dataset.db\")\n",
    "dataset_engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>tbl_name</th>\n",
       "      <th>rootpage</th>\n",
       "      <th>sql</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>table</td>\n",
       "      <td>by</td>\n",
       "      <td>by</td>\n",
       "      <td>2</td>\n",
       "      <td>CREATE TABLE \"by\" (\\n\\t\"index\" BIGINT, \\n\\tlic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>table</td>\n",
       "      <td>by-sa</td>\n",
       "      <td>by-sa</td>\n",
       "      <td>28</td>\n",
       "      <td>CREATE TABLE \"by-sa\" (\\n\\t\"index\" BIGINT, \\n\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>table</td>\n",
       "      <td>by-nc</td>\n",
       "      <td>by-nc</td>\n",
       "      <td>410</td>\n",
       "      <td>CREATE TABLE \"by-nc\" (\\n\\t\"index\" BIGINT, \\n\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>table</td>\n",
       "      <td>by-nc-sa</td>\n",
       "      <td>by-nc-sa</td>\n",
       "      <td>436</td>\n",
       "      <td>CREATE TABLE \"by-nc-sa\" (\\n\\t\"index\" BIGINT, \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>table</td>\n",
       "      <td>by-nd</td>\n",
       "      <td>by-nd</td>\n",
       "      <td>1223</td>\n",
       "      <td>CREATE TABLE \"by-nd\" (\\n\\t\"index\" BIGINT, \\n\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>table</td>\n",
       "      <td>by-nc-nd</td>\n",
       "      <td>by-nc-nd</td>\n",
       "      <td>1590</td>\n",
       "      <td>CREATE TABLE \"by-nc-nd\" (\\n\\t\"index\" BIGINT, \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>table</td>\n",
       "      <td>publicdomain</td>\n",
       "      <td>publicdomain</td>\n",
       "      <td>803</td>\n",
       "      <td>CREATE TABLE publicdomain (\\n\\t\"index\" BIGINT,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type          name      tbl_name  rootpage  \\\n",
       "0  table            by            by         2   \n",
       "1  table         by-sa         by-sa        28   \n",
       "2  table         by-nc         by-nc       410   \n",
       "3  table      by-nc-sa      by-nc-sa       436   \n",
       "4  table         by-nd         by-nd      1223   \n",
       "5  table      by-nc-nd      by-nc-nd      1590   \n",
       "6  table  publicdomain  publicdomain       803   \n",
       "\n",
       "                                                 sql  \n",
       "0  CREATE TABLE \"by\" (\\n\\t\"index\" BIGINT, \\n\\tlic...  \n",
       "1  CREATE TABLE \"by-sa\" (\\n\\t\"index\" BIGINT, \\n\\t...  \n",
       "2  CREATE TABLE \"by-nc\" (\\n\\t\"index\" BIGINT, \\n\\t...  \n",
       "3  CREATE TABLE \"by-nc-sa\" (\\n\\t\"index\" BIGINT, \\...  \n",
       "4  CREATE TABLE \"by-nd\" (\\n\\t\"index\" BIGINT, \\n\\t...  \n",
       "5  CREATE TABLE \"by-nc-nd\" (\\n\\t\"index\" BIGINT, \\...  \n",
       "6  CREATE TABLE publicdomain (\\n\\t\"index\" BIGINT,...  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = pd.read_sql(\"SELECT * FROM sqlite_master WHERE type = 'table'\", dataset_engine)\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>license</th>\n",
       "      <th>url</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>licenses/by-nc/1.0</td>\n",
       "      <td>https://serc.carleton.edu/details/images/31960...</td>\n",
       "      <td>figure 1.0 figure 1.0 View Original Image at F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3763</th>\n",
       "      <td>licenses/by-nc/2.0</td>\n",
       "      <td>https://guides.library.queensu.ca/c.php?g=7047...</td>\n",
       "      <td>How to Apply a Creative Commons License to You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7095</th>\n",
       "      <td>licenses/by-nc-nd/3.0</td>\n",
       "      <td>https://www.compadre.org/osp/bulletinboard/TDe...</td>\n",
       "      <td>Released under a  Creative Commons Attribution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5232</th>\n",
       "      <td>licenses/by-nc-sa/3.0</td>\n",
       "      <td>https://www.w3.org/Submission/ccREL/</td>\n",
       "      <td>ccREL: The Creative Commons Rights Expression\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>licenses/by-nc/1.0</td>\n",
       "      <td>https://www.google.com/help/legalnotices_maps/</td>\n",
       "      <td>Legal Notices for Google Maps/Google Earth and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    license  \\\n",
       "3294     licenses/by-nc/1.0   \n",
       "3763     licenses/by-nc/2.0   \n",
       "7095  licenses/by-nc-nd/3.0   \n",
       "5232  licenses/by-nc-sa/3.0   \n",
       "2820     licenses/by-nc/1.0   \n",
       "\n",
       "                                                    url  \\\n",
       "3294  https://serc.carleton.edu/details/images/31960...   \n",
       "3763  https://guides.library.queensu.ca/c.php?g=7047...   \n",
       "7095  https://www.compadre.org/osp/bulletinboard/TDe...   \n",
       "5232               https://www.w3.org/Submission/ccREL/   \n",
       "2820     https://www.google.com/help/legalnotices_maps/   \n",
       "\n",
       "                                               contents  \n",
       "3294  figure 1.0 figure 1.0 View Original Image at F...  \n",
       "3763  How to Apply a Creative Commons License to You...  \n",
       "7095  Released under a  Creative Commons Attribution...  \n",
       "5232  ccREL: The Creative Commons Rights Expression\\...  \n",
       "2820  Legal Notices for Google Maps/Google Earth and...  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages_dataset = pd.concat(\n",
    "    [\n",
    "        pd.read_sql(f\"SELECT * FROM '{table_name}'\", dataset_engine)\n",
    "        for table_name in tables[\"name\"]\n",
    "    ]\n",
    ")\n",
    "webpages_dataset = webpages_dataset\\\n",
    "    .loc[webpages_dataset[\"contents\"] != \"\", :]\\\n",
    "    .reset_index()\\\n",
    "    .drop([\"index\", \"title\", \"level_0\"], axis=1)\n",
    "webpages_dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>license</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1956</td>\n",
       "      <td>1956</td>\n",
       "      <td>1956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1956</td>\n",
       "      <td>39</td>\n",
       "      <td>1898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>http://agroportal.lirmm.fr/ontologies/PCO</td>\n",
       "      <td>licenses/by/4.0</td>\n",
       "      <td>403 Forbidden 403 Forbidden nginx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              url          license  \\\n",
       "count                                        1956             1956   \n",
       "unique                                       1956               39   \n",
       "top     http://agroportal.lirmm.fr/ontologies/PCO  licenses/by/4.0   \n",
       "freq                                            1              136   \n",
       "\n",
       "                                 contents  \n",
       "count                                1956  \n",
       "unique                               1898  \n",
       "top     403 Forbidden 403 Forbidden nginx  \n",
       "freq                                   25  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages_dataset_deduplicate = webpages_dataset.groupby(\"url\").first()\\\n",
    "    .reset_index()\n",
    "webpages_dataset_deduplicate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>license</th>\n",
       "      <th>Tool Typing</th>\n",
       "      <th>General Typing</th>\n",
       "      <th>Version</th>\n",
       "      <th>by</th>\n",
       "      <th>sa</th>\n",
       "      <th>nc</th>\n",
       "      <th>nd</th>\n",
       "      <th>neither</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>licenses/by/1.0</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>licenses/by/2.0</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>licenses/by/2.1</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>2.1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>licenses/by/2.5</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>2.5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>licenses/by/3.0</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>licenses/by/4.0</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            license Tool Typing General Typing  Version    by     sa     nc  \\\n",
       "0   licenses/by/1.0    licenses             by      1.0  True  False  False   \n",
       "14  licenses/by/2.0    licenses             by      2.0  True  False  False   \n",
       "27  licenses/by/2.1    licenses             by      2.1  True  False  False   \n",
       "33  licenses/by/2.5    licenses             by      2.5  True  False  False   \n",
       "39  licenses/by/3.0    licenses             by      3.0  True  False  False   \n",
       "45  licenses/by/4.0    licenses             by      4.0  True  False  False   \n",
       "\n",
       "       nd  neither  \n",
       "0   False    False  \n",
       "14  False    False  \n",
       "27  False    False  \n",
       "33  False    False  \n",
       "39  False    False  \n",
       "45  False    False  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataset_sampling\n",
    "license_map = dataset_sampling.get_license_map()\n",
    "license_ser = pd.concat([v for v in license_map.values()])\n",
    "license_ser_splits_df = license_ser.str.split(\"/\", expand=True)\n",
    "license_ser_splits_df = license_ser_splits_df.rename(\n",
    "    columns = {\n",
    "        0: \"Tool Typing\",\n",
    "        1: \"General Typing\",\n",
    "        2: \"Version\",\n",
    "        3: \"Jurisdiction\"\n",
    "    }\n",
    ")\n",
    "license_ser_splits_df[\"General Typing\"] = license_ser_splits_df[\"General Typing\"].str.replace(\"mark|zero\", \"publicdomain\", regex=True)\n",
    "license_ser_splits_df[\"General Typing\"] = license_ser_splits_df[\"General Typing\"].str.replace(\"by-nd-nc\", \"by-nc-nd\", regex=True)\n",
    "license_ser_splits_df[\"Version\"] = license_ser_splits_df[\"Version\"].astype(float)\n",
    "license_one_hot_encoding = pd.DataFrame()\n",
    "license_one_hot_encoding[\"by\"] = license_ser_splits_df[\"General Typing\"].str.contains(\"by\")\n",
    "license_one_hot_encoding[\"sa\"] = license_ser_splits_df[\"General Typing\"].str.contains(\"sa\")\n",
    "license_one_hot_encoding[\"nc\"] = license_ser_splits_df[\"General Typing\"].str.contains(\"nc\")\n",
    "license_one_hot_encoding[\"nd\"] = license_ser_splits_df[\"General Typing\"].str.contains(\"nd\")\n",
    "license_not_six_type = license_ser_splits_df[\"General Typing\"].str.contains(\"by|sa|nc|nd\")\n",
    "license_one_hot_encoding[\"neither\"] = ~(license_not_six_type.fillna(False))\n",
    "license_df = pd.concat([license_ser, license_ser_splits_df, license_one_hot_encoding], axis = 1)\\\n",
    "    .rename(columns = {0: \"license\"})\n",
    "license_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>license</th>\n",
       "      <th>contents</th>\n",
       "      <th>Tool Typing</th>\n",
       "      <th>General Typing</th>\n",
       "      <th>Version</th>\n",
       "      <th>by</th>\n",
       "      <th>sa</th>\n",
       "      <th>nc</th>\n",
       "      <th>nd</th>\n",
       "      <th>neither</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>https://research.acer.edu.au/talis/2/</td>\n",
       "      <td>licenses/by-nc-sa/3.0</td>\n",
       "      <td>\"Australian teachers and the learning environm...</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by-nc-sa</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>https://viva.pressbooks.pub/introbio2/chapter/...</td>\n",
       "      <td>licenses/by/2.1</td>\n",
       "      <td>6.2 Evolution of Simple Multicellularity – VCU...</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>2.1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>https://mdsoar.org/handle/11603/20196</td>\n",
       "      <td>publicdomain/mark/1.0</td>\n",
       "      <td>Dynamic Linkages Among U.S. Real Estate Sector...</td>\n",
       "      <td>publicdomain</td>\n",
       "      <td>publicdomain</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>https://www.ebi.ac.uk/ols/ontologies/bto</td>\n",
       "      <td>licenses/by-nd/1.0</td>\n",
       "      <td>The BRENDA Tissue Ontology (BTO) &lt; Ontology Lo...</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by-nd</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>https://spiral.imperial.ac.uk/handle/10044/1/5...</td>\n",
       "      <td>publicdomain/mark/1.0</td>\n",
       "      <td>Spiral: Targeting cattle for malaria eliminati...</td>\n",
       "      <td>publicdomain</td>\n",
       "      <td>publicdomain</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "1613              https://research.acer.edu.au/talis/2/   \n",
       "1169  https://viva.pressbooks.pub/introbio2/chapter/...   \n",
       "650               https://mdsoar.org/handle/11603/20196   \n",
       "1766           https://www.ebi.ac.uk/ols/ontologies/bto   \n",
       "688   https://spiral.imperial.ac.uk/handle/10044/1/5...   \n",
       "\n",
       "                    license  \\\n",
       "1613  licenses/by-nc-sa/3.0   \n",
       "1169        licenses/by/2.1   \n",
       "650   publicdomain/mark/1.0   \n",
       "1766     licenses/by-nd/1.0   \n",
       "688   publicdomain/mark/1.0   \n",
       "\n",
       "                                               contents   Tool Typing  \\\n",
       "1613  \"Australian teachers and the learning environm...      licenses   \n",
       "1169  6.2 Evolution of Simple Multicellularity – VCU...      licenses   \n",
       "650   Dynamic Linkages Among U.S. Real Estate Sector...  publicdomain   \n",
       "1766  The BRENDA Tissue Ontology (BTO) < Ontology Lo...      licenses   \n",
       "688   Spiral: Targeting cattle for malaria eliminati...  publicdomain   \n",
       "\n",
       "     General Typing  Version     by     sa     nc     nd  neither  \n",
       "1613       by-nc-sa      3.0   True   True   True  False    False  \n",
       "1169             by      2.1   True  False  False  False    False  \n",
       "650    publicdomain      1.0  False  False  False  False     True  \n",
       "1766          by-nd      1.0   True  False  False   True    False  \n",
       "688    publicdomain      1.0  False  False  False  False     True  "
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages_dataset_deduplicate = webpages_dataset_deduplicate.merge(license_df, on = \"license\")\n",
    "webpages_dataset_deduplicate.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>license</th>\n",
       "      <th>contents</th>\n",
       "      <th>Tool Typing</th>\n",
       "      <th>Version</th>\n",
       "      <th>by</th>\n",
       "      <th>sa</th>\n",
       "      <th>nc</th>\n",
       "      <th>nd</th>\n",
       "      <th>neither</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>General Typing</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>751</td>\n",
       "      <td>751</td>\n",
       "      <td>751</td>\n",
       "      <td>751</td>\n",
       "      <td>751</td>\n",
       "      <td>751</td>\n",
       "      <td>751</td>\n",
       "      <td>751</td>\n",
       "      <td>751</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by-nc</th>\n",
       "      <td>242</td>\n",
       "      <td>242</td>\n",
       "      <td>242</td>\n",
       "      <td>242</td>\n",
       "      <td>242</td>\n",
       "      <td>242</td>\n",
       "      <td>242</td>\n",
       "      <td>242</td>\n",
       "      <td>242</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by-nc-nd</th>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by-nc-sa</th>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by-nd</th>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by-sa</th>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publicdomain</th>\n",
       "      <td>234</td>\n",
       "      <td>234</td>\n",
       "      <td>234</td>\n",
       "      <td>234</td>\n",
       "      <td>234</td>\n",
       "      <td>234</td>\n",
       "      <td>234</td>\n",
       "      <td>234</td>\n",
       "      <td>234</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                url  license  contents  Tool Typing  Version   by   sa   nc  \\\n",
       "General Typing                                                                \n",
       "by              751      751       751          751      751  751  751  751   \n",
       "by-nc           242      242       242          242      242  242  242  242   \n",
       "by-nc-nd        117      117       117          117      117  117  117  117   \n",
       "by-nc-sa        201      201       201          201      201  201  201  201   \n",
       "by-nd           131      131       131          131      131  131  131  131   \n",
       "by-sa           280      280       280          280      280  280  280  280   \n",
       "publicdomain    234      234       234          234      234  234  234  234   \n",
       "\n",
       "                 nd  neither  \n",
       "General Typing                \n",
       "by              751      751  \n",
       "by-nc           242      242  \n",
       "by-nc-nd        117      117  \n",
       "by-nc-sa        201      201  \n",
       "by-nd           131      131  \n",
       "by-sa           280      280  \n",
       "publicdomain    234      234  "
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages_dataset_deduplicate.groupby(\"General Typing\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#webpages_dataset_deduplicate[\"contents\"] = webpages_dataset_deduplicate[\"contents\"].apply(lambda x: x[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_unicodes(ser):\n",
    "    return ser.map(lambda x: \" \".join([c for c in x if c in string.printable]))\n",
    "\n",
    "def has_unicodes(s, tolerance = 30):\n",
    "    return np.sum([c not in string.printable for c in s]) <= tolerance\n",
    "\n",
    "def not_well_decrypted(s, tolerance = 25):\n",
    "    words = re.split(r\"\\s+\", s)\n",
    "    return np.sum([len(c) == 1 for c in words]) <= tolerance\n",
    "\n",
    "def remove_unicodes_aggressive(df, field_name = \"contents\"):\n",
    "    df_remove_unicode = df.loc[df[field_name].apply(has_unicodes), :]\n",
    "    return df_remove_unicode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Less Aggressive pruning\n",
    "#webpages_dataset_deduplicate[\"parsed_contents\"] = remove_unicodes(webpages_dataset_deduplicate[\"contents\"])\n",
    "#webpages_dataset_deduplicate.loc[1149, [\"contents\", \"parsed_contents\"]]\n",
    "\n",
    "# More Aggressive pruning\n",
    "webpages_dataset_deduplicate = remove_unicodes_aggressive(webpages_dataset_deduplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_less_than_c_chars(s, tolerance = 2):\n",
    "    words = re.split(r\"\\s+\", s)\n",
    "    return \" \".join([c for c in words if len(c) > tolerance])\n",
    "\n",
    "def remove_more_than_c_chars(s, tolerance = 15):\n",
    "    words = re.split(r\"\\s+\", s)\n",
    "    return \" \".join([c for c in words if len(c) <= tolerance])\n",
    "\n",
    "def remove_non_english(s):\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    return \" \".join(w for w in re.split(r\"\\s+\", s) if w in words)\n",
    "\n",
    "def remove_web_urls(s):\n",
    "    return re.sub(r\"[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", \" \", s)\n",
    "\n",
    "def clear_stopwords(s):\n",
    "    return \" \".join([word for word in re.split(r\"\\s+\", s) if word.lower() not in nltk.corpus.stopwords.words('english')])\n",
    "\n",
    "def clear_stopwords_series(ser):\n",
    "    return ser.apply(clear_stopwords)\n",
    "\n",
    "def overall_cleaning(ser):\n",
    "    cleaned_ser = ser.str.lower()\n",
    "    cleaned_ser = cleaned_ser.apply(remove_web_urls)\n",
    "    cleaned_ser = cleaned_ser.str.replace(r'[^A-Za-z\\s]', ' ', regex = True)\n",
    "    cleaned_ser = cleaned_ser.str.replace(r\"\\s+\", \" \", regex = True)\n",
    "    cleaned_ser = cleaned_ser.apply(clear_stopwords)\n",
    "    cleaned_ser = cleaned_ser.apply(remove_less_than_c_chars)\n",
    "    cleaned_ser = cleaned_ser.apply(remove_non_english)\n",
    "    cleaned_ser = cleaned_ser.apply(clear_stopwords)\n",
    "    #cleaned_ser = cleaned_ser.apply(remove_more_than_c_chars)\n",
    "    return cleaned_ser\n",
    "\n",
    "webpages_dataset_deduplicate[\"cleaned_contents\"] = overall_cleaning(webpages_dataset_deduplicate[\"contents\"])\n",
    "webpages_dataset_deduplicate = webpages_dataset_deduplicate.loc[\n",
    "    webpages_dataset_deduplicate[\"cleaned_contents\"].str.len() >= 500, :\n",
    "]\n",
    "#webpages_dataset_deduplicate[\"parsed_cleaned_contents\"] = clear_stopwords_series(webpages_dataset_deduplicate[\"cleaned_contents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry content:\n",
      "language maintenance cultural identity heritage university home university college theses view item disabled browser site may work without language maintenance cultural identity heritage duro advisor language learning heritage culture identity word cultural competence identity heritage date publisher university work license free copy redistribute material format well remix transform build upon material long give appropriate credit original creator provide link license indicate made may use work commercial purpose must distribute identical licen\n",
      "\n",
      "Entry content:\n",
      "certainly comfort zone qualitative study dementia care educational needs study current research staff teaching learning alumni library iris certainly comfort zone qualitative study dementia care educational needs cora home college medicine health general practice general practice journal view item disabled browser site may work without item name size format description version view open certainly comfort zone qualitative study dementia care educational needs tony aisling henry date copyright author open access article distributed creative commo\n",
      "\n",
      "Entry content:\n",
      "node data system node concept standardized assessment lymph cancer skip main content advertisement search cart log search search node data system node concept standardized assessment lymph cancer oncology open access node data system node concept standardized assessment lymph cancer becker tan show radiology volume cite article metrics correction article march article abstract node lack consensus radiologic assessment lymph node involvement cancer increasing demand structured likelihood disease involvement node data system node systematically d\n",
      "\n",
      "Entry content:\n",
      "diabetes metabolism journal home submission contact home scope journal editorial board best practice metrics contact browse current issue ahead print article category article topic special thematic fact covid best paper year research publication ethics article charge best material permission copyright permission submission search journal page path home journal diabetes metabolism journal official journal diabetes association bimonthly accepted process peer review official title journal diabetes metabolism journal title diabetes journal title jo\n",
      "\n",
      "Entry content:\n",
      "creative commons attribution noncommercial marine information jump navigation home creative commons attribution noncommercial see free share copy redistribute material medium format adapt remix transform build upon material licensor cannot revoke long follow license following attribution must give appropriate credit provide link license indicate made may reasonable manner way licensor use noncommercial may use material commercial additional may apply legal technological legally restrict anything license link creative commons relative visibility\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in webpages_dataset_deduplicate[\"cleaned_contents\"].sample(5):\n",
    "    print(\n",
    "        f\"Entry content:\\n{row[:550]}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'creativecommons org licenses by sa  '"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_url(url):\n",
    "    return \" \".join(re.split(r\"[_/\\.-]\", re.sub(r\"\\d\", \"\", url)))\n",
    "tokenize_url(\"creativecommons.org/licenses/by-sa/4.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = webpages_dataset_deduplicate.copy()\n",
    "dataset['token_url'] = dataset[\"url\"].apply(tokenize_url)\n",
    "dataset[\"train_text\"] = dataset[\"token_url\"] + \" \" + dataset[\"cleaned_contents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features_tfidf(train, test, text_field = \"train_text\", svd = True):\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        use_idf=True, stop_words=\"english\", max_features = 5000, sublinear_tf=True\n",
    "    )\n",
    "    tfidf_vectorizer.fit(train[text_field].values)\n",
    "    train_vectorized = tfidf_vectorizer.transform(train[text_field].values)\n",
    "    test_vectorized = tfidf_vectorizer.transform(test[text_field].values)\n",
    "    if svd:\n",
    "        tsvd = TruncatedSVD(n_components = 150)\n",
    "        tsvd.fit(train_vectorized)\n",
    "        train_vectorized = tsvd.transform(train_vectorized)\n",
    "        test_vectorized = tsvd.transform(test_vectorized)\n",
    "    return train_vectorized, test_vectorized, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    605\n",
       "1    190\n",
       "2     83\n",
       "3    162\n",
       "4     95\n",
       "5    199\n",
       "6    189\n",
       "Name: url, dtype: int64"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby(\"General Typing\")[[\"url\"]].count().reset_index()[\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dataset = pd.DataFrame()\n",
    "dataset_counts = dataset.groupby(\"General Typing\")[[\"url\"]].count().reset_index()[\"url\"]\n",
    "license_dict = {\n",
    "        \"by\": 0,\n",
    "        \"by-nc\": 1,\n",
    "        \"by-nc-nd\": 2,\n",
    "        \"by-nc-sa\": 3,\n",
    "        \"by-nd\": 4,\n",
    "        \"by-sa\": 5,\n",
    "        \"publicdomain\": 6\n",
    "}\n",
    "\n",
    "model_dataset[\"train_text\"], model_dataset[\"General Typing\"] = dataset[\"cleaned_contents\"], dataset[\"General Typing\"]\n",
    "model_dataset[\"General Typing\"].replace(\n",
    "    license_dict,\n",
    "    inplace = True\n",
    ")\n",
    "\n",
    "training_set, test_set = train_test_split(model_dataset, test_size = 0.2)\n",
    "Y_train = training_set[\"General Typing\"].values\n",
    "Y_test = test_set[\"General Typing\"].values\n",
    "X_train, X_test, model_vecter = extract_text_features_tfidf(\n",
    "    training_set, test_set\n",
    ")\n",
    "\n",
    "smote = SMOTE(\n",
    "    sampling_strategy={\n",
    "        k: int(0.8 * min(round(np.mean(dataset_counts.values)), dataset_counts.iloc[k] * 1.4))\n",
    "        for k in range(1, 7)} #\n",
    ")\n",
    "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logreg = LogisticRegression(\n",
    "    verbose = 0,\n",
    "    penalty = 'l2',\n",
    "    solver = \"liblinear\",\n",
    "    class_weight = \"balanced\",\n",
    "    C = 0.05\n",
    ").fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3698529411764706 \n",
      " 0.6764705882352942 \n",
      " 0.8242647058823529 \n",
      "==============\n",
      " 0.4786885245901639 \n",
      " 0.7081967213114754 \n",
      " 0.8163934426229508\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    top_k_accuracy_score(Y_train, model_logreg.predict_proba(X_train), k = 1),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_train, model_logreg.predict_proba(X_train), k = 2),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_train, model_logreg.predict_proba(X_train), k = 3),\n",
    "    \"\\n==============\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_logreg.predict_proba(X_test), k = 1),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_logreg.predict_proba(X_test), k = 2),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_logreg.predict_proba(X_test), k = 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "model_svc = SVC(\n",
    "    C = 0.5,\n",
    "    verbose = 1,\n",
    "    probability = True,\n",
    "    kernel = \"poly\",\n",
    "    degree = 1,\n",
    "    class_weight = 'balanced'\n",
    ").fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5625 \n",
      " 0.7867647058823529 \n",
      " 0.8985294117647059 \n",
      "==============\n",
      " 0.5081967213114754 \n",
      " 0.7016393442622951 \n",
      " 0.7934426229508197\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    top_k_accuracy_score(Y_train, model_svc.predict_proba(X_train), k = 1),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_train, model_svc.predict_proba(X_train), k = 2),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_train, model_svc.predict_proba(X_train), k = 3),\n",
    "    \"\\n==============\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_svc.predict_proba(X_test), k = 1),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_svc.predict_proba(X_test), k = 2),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_svc.predict_proba(X_test), k = 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2'\n",
    "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 16\n",
    "TRAINING_RATIO = 0.7\n",
    "\n",
    "model_dataset = dataset.loc[:, [\"cleaned_contents\", \"General Typing\"]]\n",
    "model_dataset[\"General Typing\"].replace(\n",
    "    license_dict,\n",
    "    inplace = True\n",
    ")\n",
    "\n",
    "model_dataset_copy = model_dataset[\n",
    "    ~model_dataset[\"General Typing\"].isin([2, 4])\n",
    "]\n",
    "\n",
    "target = model_dataset_copy.pop(\"General Typing\")\n",
    "# dataset_tensor = tf.convert_to_tensor(model_dataset, dtype=np.string_)\n",
    "tf_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((model_dataset_copy, target))\n",
    "    .shuffle(50)\n",
    ")\n",
    "train_dataset = tf_dataset.take(int(TRAINING_RATIO * len(tf_dataset)))\n",
    "test_dataset = tf_dataset.skip(int(TRAINING_RATIO * len(tf_dataset)))\n",
    "val_dataset = test_dataset.skip(\n",
    "    int((1 - TRAINING_RATIO) * 0.5 * len(tf_dataset))\n",
    ")\n",
    "test_dataset = test_dataset.take(\n",
    "    int((1 - TRAINING_RATIO) * 0.5 * len(tf_dataset))\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='parsed_cleaned_contents')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.3)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "59/59 [==============================] - 96s 2s/step - loss: 2.8529e-07 - accuracy: 0.2742 - f1_score: 1.4106 - val_loss: 2.6426e-07 - val_accuracy: 0.2759 - val_f1_score: 1.3783\n",
      "Epoch 2/10\n",
      "59/59 [==============================] - 90s 2s/step - loss: 2.8491e-07 - accuracy: 0.2954 - f1_score: 1.4100 - val_loss: 2.6484e-07 - val_accuracy: 0.2611 - val_f1_score: 1.3792\n",
      "Epoch 3/10\n",
      "59/59 [==============================] - ETA: 0s - loss: 2.8415e-07 - accuracy: 0.2880 - f1_score: 1.4089"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\UCBF22\\DSD Fall 2022\\model_sampling\\model_dev.ipynb Cell 38\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m classifier_model \u001b[39m=\u001b[39m build_classifier_model()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m classifier_model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m, F1Score(\u001b[39m1\u001b[39m, average \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m history \u001b[39m=\u001b[39m classifier_model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train_dataset,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py:1420\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1407\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[0;32m   1408\u001b[0m       x\u001b[39m=\u001b[39mval_x,\n\u001b[0;32m   1409\u001b[0m       y\u001b[39m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1418\u001b[0m       model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m   1419\u001b[0m       steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution)\n\u001b[1;32m-> 1420\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m   1421\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[0;32m   1422\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[0;32m   1423\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[0;32m   1424\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[0;32m   1425\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   1426\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1427\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1428\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1429\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1430\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1431\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1432\u001b[0m val_logs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m   1433\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py:1716\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1714\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1715\u001b[0m   callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1716\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_function(iterator)\n\u001b[0;32m   1717\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1718\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2957\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1854\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "classifier_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\"accuracy\", F1Score(1, average = \"weighted\")]\n",
    ")\n",
    "history = classifier_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd98d7e735064cd82a69955e27f6d0d35e8c81af503a9f702e870f7c3b7bf96a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
