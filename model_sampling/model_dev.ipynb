{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\brand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\brand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "import os\n",
    "import re\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x17419e53760>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CWD = os.getcwd()\n",
    "dataset_engine = sqlalchemy.create_engine(f\"sqlite:///{CWD}/modeling_dataset.db\")\n",
    "dataset_engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>tbl_name</th>\n",
       "      <th>rootpage</th>\n",
       "      <th>sql</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>table</td>\n",
       "      <td>by</td>\n",
       "      <td>by</td>\n",
       "      <td>2</td>\n",
       "      <td>CREATE TABLE \"by\" (\\n\\t\"index\" BIGINT, \\n\\tlic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>table</td>\n",
       "      <td>by-sa</td>\n",
       "      <td>by-sa</td>\n",
       "      <td>28</td>\n",
       "      <td>CREATE TABLE \"by-sa\" (\\n\\t\"index\" BIGINT, \\n\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>table</td>\n",
       "      <td>by-nc</td>\n",
       "      <td>by-nc</td>\n",
       "      <td>410</td>\n",
       "      <td>CREATE TABLE \"by-nc\" (\\n\\t\"index\" BIGINT, \\n\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>table</td>\n",
       "      <td>by-nc-sa</td>\n",
       "      <td>by-nc-sa</td>\n",
       "      <td>436</td>\n",
       "      <td>CREATE TABLE \"by-nc-sa\" (\\n\\t\"index\" BIGINT, \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>table</td>\n",
       "      <td>by-nd</td>\n",
       "      <td>by-nd</td>\n",
       "      <td>1223</td>\n",
       "      <td>CREATE TABLE \"by-nd\" (\\n\\t\"index\" BIGINT, \\n\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>table</td>\n",
       "      <td>by-nc-nd</td>\n",
       "      <td>by-nc-nd</td>\n",
       "      <td>1590</td>\n",
       "      <td>CREATE TABLE \"by-nc-nd\" (\\n\\t\"index\" BIGINT, \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>table</td>\n",
       "      <td>publicdomain</td>\n",
       "      <td>publicdomain</td>\n",
       "      <td>803</td>\n",
       "      <td>CREATE TABLE publicdomain (\\n\\t\"index\" BIGINT,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type          name      tbl_name  rootpage  \\\n",
       "0  table            by            by         2   \n",
       "1  table         by-sa         by-sa        28   \n",
       "2  table         by-nc         by-nc       410   \n",
       "3  table      by-nc-sa      by-nc-sa       436   \n",
       "4  table         by-nd         by-nd      1223   \n",
       "5  table      by-nc-nd      by-nc-nd      1590   \n",
       "6  table  publicdomain  publicdomain       803   \n",
       "\n",
       "                                                 sql  \n",
       "0  CREATE TABLE \"by\" (\\n\\t\"index\" BIGINT, \\n\\tlic...  \n",
       "1  CREATE TABLE \"by-sa\" (\\n\\t\"index\" BIGINT, \\n\\t...  \n",
       "2  CREATE TABLE \"by-nc\" (\\n\\t\"index\" BIGINT, \\n\\t...  \n",
       "3  CREATE TABLE \"by-nc-sa\" (\\n\\t\"index\" BIGINT, \\...  \n",
       "4  CREATE TABLE \"by-nd\" (\\n\\t\"index\" BIGINT, \\n\\t...  \n",
       "5  CREATE TABLE \"by-nc-nd\" (\\n\\t\"index\" BIGINT, \\...  \n",
       "6  CREATE TABLE publicdomain (\\n\\t\"index\" BIGINT,...  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables = pd.read_sql(\"SELECT * FROM sqlite_master WHERE type = 'table'\", dataset_engine)\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>license</th>\n",
       "      <th>url</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8162</th>\n",
       "      <td>licenses/by-nc-nd/3.0</td>\n",
       "      <td>https://learning.hccs.edu/faculty/elisa.diehl/...</td>\n",
       "      <td>Sound Credits for The Spoilers 2022 — HCC Lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>licenses/by-sa/2.0</td>\n",
       "      <td>https://github.com/beatrizmilz/blog-en/blob/ma...</td>\n",
       "      <td>blog-en/_quarto.yml at main · beatrizmilz/blog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6016</th>\n",
       "      <td>licenses/by-nd/2.5</td>\n",
       "      <td>https://www.lawfareblog.com/justice-department...</td>\n",
       "      <td>The Justice Department Indicted Russian Nation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4220</th>\n",
       "      <td>licenses/by-nc-sa/2.5</td>\n",
       "      <td>https://www.mediawiki.org/wiki/Module:Extensio...</td>\n",
       "      <td>Module:Extension/sandbox - MediaWiki Jump to c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7404</th>\n",
       "      <td>licenses/by-nd-nc/2.0</td>\n",
       "      <td>https://libguides.colorado.edu/c.php?g=887144&amp;...</td>\n",
       "      <td>CC Licenses - Creative Commons - Research Guid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    license  \\\n",
       "8162  licenses/by-nc-nd/3.0   \n",
       "2320     licenses/by-sa/2.0   \n",
       "6016     licenses/by-nd/2.5   \n",
       "4220  licenses/by-nc-sa/2.5   \n",
       "7404  licenses/by-nd-nc/2.0   \n",
       "\n",
       "                                                    url  \\\n",
       "8162  https://learning.hccs.edu/faculty/elisa.diehl/...   \n",
       "2320  https://github.com/beatrizmilz/blog-en/blob/ma...   \n",
       "6016  https://www.lawfareblog.com/justice-department...   \n",
       "4220  https://www.mediawiki.org/wiki/Module:Extensio...   \n",
       "7404  https://libguides.colorado.edu/c.php?g=887144&...   \n",
       "\n",
       "                                               contents  \n",
       "8162  Sound Credits for The Spoilers 2022 — HCC Lear...  \n",
       "2320  blog-en/_quarto.yml at main · beatrizmilz/blog...  \n",
       "6016  The Justice Department Indicted Russian Nation...  \n",
       "4220  Module:Extension/sandbox - MediaWiki Jump to c...  \n",
       "7404  CC Licenses - Creative Commons - Research Guid...  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages_dataset = pd.concat(\n",
    "    [\n",
    "        pd.read_sql(f\"SELECT * FROM '{table_name}'\", dataset_engine)\n",
    "        for table_name in tables[\"name\"]\n",
    "    ]\n",
    ")\n",
    "webpages_dataset = webpages_dataset\\\n",
    "    .loc[webpages_dataset[\"contents\"] != \"\", :]\\\n",
    "    .reset_index()\\\n",
    "    .drop([\"index\", \"title\", \"level_0\"], axis=1)\n",
    "webpages_dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>license</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3238</td>\n",
       "      <td>3238</td>\n",
       "      <td>3238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3238</td>\n",
       "      <td>39</td>\n",
       "      <td>3123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>http://abcr-mefmo.org/index.php/abcr/about</td>\n",
       "      <td>licenses/by/2.1</td>\n",
       "      <td>403 Forbidden 403 Forbidden nginx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               url          license  \\\n",
       "count                                         3238             3238   \n",
       "unique                                        3238               39   \n",
       "top     http://abcr-mefmo.org/index.php/abcr/about  licenses/by/2.1   \n",
       "freq                                             1              184   \n",
       "\n",
       "                                 contents  \n",
       "count                                3238  \n",
       "unique                               3123  \n",
       "top     403 Forbidden 403 Forbidden nginx  \n",
       "freq                                   44  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages_dataset_deduplicate = webpages_dataset.groupby(\"url\").first()\\\n",
    "    .reset_index()\n",
    "webpages_dataset_deduplicate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>license</th>\n",
       "      <th>Tool Typing</th>\n",
       "      <th>General Typing</th>\n",
       "      <th>Version</th>\n",
       "      <th>by</th>\n",
       "      <th>sa</th>\n",
       "      <th>nc</th>\n",
       "      <th>nd</th>\n",
       "      <th>neither</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>licenses/by/1.0</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>licenses/by/2.0</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>licenses/by/2.1</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>2.1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>licenses/by/2.5</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>2.5</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>licenses/by/3.0</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>licenses/by/4.0</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            license Tool Typing General Typing  Version    by     sa     nc  \\\n",
       "0   licenses/by/1.0    licenses             by      1.0  True  False  False   \n",
       "14  licenses/by/2.0    licenses             by      2.0  True  False  False   \n",
       "27  licenses/by/2.1    licenses             by      2.1  True  False  False   \n",
       "33  licenses/by/2.5    licenses             by      2.5  True  False  False   \n",
       "39  licenses/by/3.0    licenses             by      3.0  True  False  False   \n",
       "45  licenses/by/4.0    licenses             by      4.0  True  False  False   \n",
       "\n",
       "       nd  neither  \n",
       "0   False    False  \n",
       "14  False    False  \n",
       "27  False    False  \n",
       "33  False    False  \n",
       "39  False    False  \n",
       "45  False    False  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataset_sampling\n",
    "license_map = dataset_sampling.get_license_map()\n",
    "license_ser = pd.concat([v for v in license_map.values()])\n",
    "license_ser_splits_df = license_ser.str.split(\"/\", expand=True)\n",
    "license_ser_splits_df = license_ser_splits_df.rename(\n",
    "    columns = {\n",
    "        0: \"Tool Typing\",\n",
    "        1: \"General Typing\",\n",
    "        2: \"Version\",\n",
    "        3: \"Jurisdiction\"\n",
    "    }\n",
    ")\n",
    "license_ser_splits_df[\"General Typing\"] = license_ser_splits_df[\"General Typing\"].str.replace(\"mark|zero\", \"publicdomain\", regex=True)\n",
    "license_ser_splits_df[\"General Typing\"] = license_ser_splits_df[\"General Typing\"].str.replace(\"by-nd-nc\", \"by-nc-nd\", regex=True)\n",
    "license_ser_splits_df[\"Version\"] = license_ser_splits_df[\"Version\"].astype(float)\n",
    "license_one_hot_encoding = pd.DataFrame()\n",
    "license_one_hot_encoding[\"by\"] = license_ser_splits_df[\"General Typing\"].str.contains(\"by\")\n",
    "license_one_hot_encoding[\"sa\"] = license_ser_splits_df[\"General Typing\"].str.contains(\"sa\")\n",
    "license_one_hot_encoding[\"nc\"] = license_ser_splits_df[\"General Typing\"].str.contains(\"nc\")\n",
    "license_one_hot_encoding[\"nd\"] = license_ser_splits_df[\"General Typing\"].str.contains(\"nd\")\n",
    "license_not_six_type = license_ser_splits_df[\"General Typing\"].str.contains(\"by|sa|nc|nd\")\n",
    "license_one_hot_encoding[\"neither\"] = ~(license_not_six_type.fillna(False))\n",
    "license_df = pd.concat([license_ser, license_ser_splits_df, license_one_hot_encoding], axis = 1)\\\n",
    "    .rename(columns = {0: \"license\"})\n",
    "license_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>license</th>\n",
       "      <th>contents</th>\n",
       "      <th>Tool Typing</th>\n",
       "      <th>General Typing</th>\n",
       "      <th>Version</th>\n",
       "      <th>by</th>\n",
       "      <th>sa</th>\n",
       "      <th>nc</th>\n",
       "      <th>nd</th>\n",
       "      <th>neither</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>https://www.leidensecurityandglobalaffairs.nl/...</td>\n",
       "      <td>licenses/by-nc/2.0</td>\n",
       "      <td>Emotional Labor During Disruptive Times: a Col...</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by-nc</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>https://www.healthcaredive.com/news/soc-teleme...</td>\n",
       "      <td>licenses/by-sa/2.0</td>\n",
       "      <td>SOC Telemed to be acquired by Healthcare Merge...</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by-sa</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>https://scholar.dominican.edu/all-faculty/170/</td>\n",
       "      <td>licenses/by-nd/3.0</td>\n",
       "      <td>\"Big History, Big Lesson\" by Mojgan Behmand an...</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by-nd</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>https://blog.to.com/advisory-webacms-2-1-0-cro...</td>\n",
       "      <td>licenses/by/2.1</td>\n",
       "      <td>Advisory: WebACMS 2.1.0 Cross-Site Scripting -...</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by</td>\n",
       "      <td>2.1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2794</th>\n",
       "      <td>https://www.esa.int/Services/Creative_Commons_...</td>\n",
       "      <td>licenses/by-sa/2.0</td>\n",
       "      <td>ESA - Creative Commons Attribution-ShareAlike ...</td>\n",
       "      <td>licenses</td>\n",
       "      <td>by-sa</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url             license  \\\n",
       "1836  https://www.leidensecurityandglobalaffairs.nl/...  licenses/by-nc/2.0   \n",
       "2799  https://www.healthcaredive.com/news/soc-teleme...  licenses/by-sa/2.0   \n",
       "2970     https://scholar.dominican.edu/all-faculty/170/  licenses/by-nd/3.0   \n",
       "2239  https://blog.to.com/advisory-webacms-2-1-0-cro...     licenses/by/2.1   \n",
       "2794  https://www.esa.int/Services/Creative_Commons_...  licenses/by-sa/2.0   \n",
       "\n",
       "                                               contents Tool Typing  \\\n",
       "1836  Emotional Labor During Disruptive Times: a Col...    licenses   \n",
       "2799  SOC Telemed to be acquired by Healthcare Merge...    licenses   \n",
       "2970  \"Big History, Big Lesson\" by Mojgan Behmand an...    licenses   \n",
       "2239  Advisory: WebACMS 2.1.0 Cross-Site Scripting -...    licenses   \n",
       "2794  ESA - Creative Commons Attribution-ShareAlike ...    licenses   \n",
       "\n",
       "     General Typing  Version    by     sa     nc     nd  neither  \n",
       "1836          by-nc      2.0  True  False   True  False    False  \n",
       "2799          by-sa      2.0  True   True  False  False    False  \n",
       "2970          by-nd      3.0  True  False  False   True    False  \n",
       "2239             by      2.1  True  False  False  False    False  \n",
       "2794          by-sa      2.0  True   True  False  False    False  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages_dataset_deduplicate = webpages_dataset_deduplicate.merge(license_df, on = \"license\")\n",
    "webpages_dataset_deduplicate.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>license</th>\n",
       "      <th>contents</th>\n",
       "      <th>Tool Typing</th>\n",
       "      <th>Version</th>\n",
       "      <th>by</th>\n",
       "      <th>sa</th>\n",
       "      <th>nc</th>\n",
       "      <th>nd</th>\n",
       "      <th>neither</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>General Typing</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>985</td>\n",
       "      <td>985</td>\n",
       "      <td>985</td>\n",
       "      <td>985</td>\n",
       "      <td>985</td>\n",
       "      <td>985</td>\n",
       "      <td>985</td>\n",
       "      <td>985</td>\n",
       "      <td>985</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by-nc</th>\n",
       "      <td>530</td>\n",
       "      <td>530</td>\n",
       "      <td>530</td>\n",
       "      <td>530</td>\n",
       "      <td>530</td>\n",
       "      <td>530</td>\n",
       "      <td>530</td>\n",
       "      <td>530</td>\n",
       "      <td>530</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by-nc-nd</th>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by-nc-sa</th>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by-nd</th>\n",
       "      <td>318</td>\n",
       "      <td>318</td>\n",
       "      <td>318</td>\n",
       "      <td>318</td>\n",
       "      <td>318</td>\n",
       "      <td>318</td>\n",
       "      <td>318</td>\n",
       "      <td>318</td>\n",
       "      <td>318</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by-sa</th>\n",
       "      <td>588</td>\n",
       "      <td>588</td>\n",
       "      <td>588</td>\n",
       "      <td>588</td>\n",
       "      <td>588</td>\n",
       "      <td>588</td>\n",
       "      <td>588</td>\n",
       "      <td>588</td>\n",
       "      <td>588</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publicdomain</th>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                url  license  contents  Tool Typing  Version   by   sa   nc  \\\n",
       "General Typing                                                                \n",
       "by              985      985       985          985      985  985  985  985   \n",
       "by-nc           530      530       530          530      530  530  530  530   \n",
       "by-nc-nd        208      208       208          208      208  208  208  208   \n",
       "by-nc-sa        360      360       360          360      360  360  360  360   \n",
       "by-nd           318      318       318          318      318  318  318  318   \n",
       "by-sa           588      588       588          588      588  588  588  588   \n",
       "publicdomain    249      249       249          249      249  249  249  249   \n",
       "\n",
       "                 nd  neither  \n",
       "General Typing                \n",
       "by              985      985  \n",
       "by-nc           530      530  \n",
       "by-nc-nd        208      208  \n",
       "by-nc-sa        360      360  \n",
       "by-nd           318      318  \n",
       "by-sa           588      588  \n",
       "publicdomain    249      249  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages_dataset_deduplicate.groupby(\"General Typing\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpages_dataset_deduplicate[\"contents\"] = webpages_dataset_deduplicate[\"contents\"].apply(lambda x: x[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_unicodes(ser):\n",
    "    return ser.map(lambda x: \" \".join([c for c in x if c in string.printable]))\n",
    "\n",
    "def has_unicodes(s, tolerance = 20):\n",
    "    return np.sum([c not in string.printable for c in s]) <= tolerance\n",
    "\n",
    "def not_well_decrypted(s, tolerance = 25):\n",
    "    words = re.split(r\"\\s+\", s)\n",
    "    return np.sum([len(c) == 1 for c in words]) <= tolerance\n",
    "\n",
    "def remove_unicodes_aggressive(df, field_name = \"contents\"):\n",
    "    df_remove_unicode = df.loc[df[field_name].apply(has_unicodes), :]\n",
    "    df_remove_unicode = df_remove_unicode.loc[\n",
    "        df_remove_unicode[field_name].str.len() >= 600, :\n",
    "    ]\n",
    "    return df_remove_unicode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Less Aggressive pruning\n",
    "webpages_dataset_deduplicate[\"parsed_contents\"] = remove_unicodes(webpages_dataset_deduplicate[\"contents\"])\n",
    "#webpages_dataset_deduplicate.loc[1149, [\"contents\", \"parsed_contents\"]]\n",
    "\n",
    "# More Aggressive pruning\n",
    "#webpages_dataset_deduplicate = remove_unicodes_aggressive(webpages_dataset_deduplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_less_than_c_chars(s, tolerance = 1):\n",
    "    words = re.split(r\"\\s+\", s)\n",
    "    return \" \".join([c for c in words if len(c) > tolerance and \"obj\" not in c])\n",
    "\n",
    "def remove_more_than_c_chars(s, tolerance = 15):\n",
    "    words = re.split(r\"\\s+\", s)\n",
    "    return \" \".join([c for c in words if len(c) <= tolerance])\n",
    "\n",
    "def remove_non_english(s):\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    return \" \".join(w for w in re.split(r\"\\s+\", s) if w in words)\n",
    "\n",
    "def remove_web_urls(s):\n",
    "    return re.sub(r\"[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", \" \", s)\n",
    "\n",
    "def clear_stopwords(s):\n",
    "    return \" \".join([word for word in re.split(r\"\\s+\", s) if word.lower() not in nltk.corpus.stopwords.words('english')])\n",
    "\n",
    "def clear_stopwords_series(ser):\n",
    "    return ser.apply(clear_stopwords)\n",
    "\n",
    "def overall_cleaning(ser):\n",
    "    cleaned_ser = ser.str.lower()\n",
    "    cleaned_ser = cleaned_ser.apply(remove_web_urls)\n",
    "    cleaned_ser = cleaned_ser.str.replace(r'[^A-Za-z\\s]', ' ', regex = True)\n",
    "    cleaned_ser = cleaned_ser.str.replace(r\"\\s+\", \" \", regex = True)\n",
    "    cleaned_ser = cleaned_ser.apply(remove_less_than_c_chars)\n",
    "    cleaned_ser = cleaned_ser.apply(remove_non_english)\n",
    "    #cleaned_ser = cleaned_ser.apply(remove_more_than_c_chars)\n",
    "    return cleaned_ser\n",
    "\n",
    "webpages_dataset_deduplicate[\"cleaned_contents\"] = overall_cleaning(webpages_dataset_deduplicate[\"contents\"])\n",
    "#webpages_dataset_deduplicate[\"parsed_cleaned_contents\"] = clear_stopwords_series(webpages_dataset_deduplicate[\"cleaned_contents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry content:\n",
      "just moment if the site connection is secure enable and to continue needs to review the security of your connection before proceeding ray id performance security by\n",
      "\n",
      "Entry content:\n",
      "profile ma ce oak ra ta ea do en\n",
      "\n",
      "Entry content:\n",
      "therapeutic resistance in early rheumatoid arthritis epigenetic annals of the rheumatic skip to main content subscribe log in more log in via institution log in via log in your and password for personal or of institutional password forgot your log in register new account forgot your user name or password basket search more search for this advanced search latest content current issue archive about search for this advanced search close more main menu latest content current issue archive about subscribe log in more log in via institution log in via log in your and password for personal or of institutional password forgot your log in register new account forgot your user name or password you are\n",
      "\n",
      "Entry content:\n",
      "investigative magnetic resonance about view full text for search and scope editorial board journal information current issue archive for research and publication ethics submission copyright transfer agreement contact us full text search journal information journal title investigative magnetic resonance journal abbreviation acronym publication date vol no frequency quarterly publisher society of magnetic resonance in medicine language prefix journal of the society of magnetic resonance in medicine broad subject term diagnostic mesh magnetic resonance magnetic resonance spectroscopy engineering radiology nuclear medicine medical open access electronic links indexed tracked covered by scholar\n",
      "\n",
      "Entry content:\n",
      "learning in covid apply giving directory alumni login login to staff portal login to learning portal search what is learning in learning in lit is collection of minute user friendly video covering at the level of the first clinical year in medical school lit can be used by all health professional to quickly introduce themselves to important and supplement their in class lecture lit have been by world class clinical faculty and each video peer review process to ensure accuracy of information how did it come about in tan sarraf and yee of duke medical school in and duke university in the the development of digital library of clinical training faculty and student over lit with the support of du\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in webpages_dataset_deduplicate[\"cleaned_contents\"].sample(5):\n",
    "    print(\n",
    "        f\"Entry content:\\n{row[:700]}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'creativecommons org licenses by sa  '"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_url(url):\n",
    "    return \" \".join(re.split(r\"[_/\\.-]\", re.sub(r\"\\d\", \"\", url)))\n",
    "tokenize_url(\"creativecommons.org/licenses/by-sa/4.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = webpages_dataset_deduplicate.copy()\n",
    "dataset['token_url'] = dataset[\"url\"].apply(tokenize_url)\n",
    "dataset[\"train_text\"] = dataset[\"token_url\"] + \" \" + dataset[\"cleaned_contents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features_tfidf(train, test, text_field = \"train_text\"):\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True, max_df=0.9, stop_words=\"english\")\n",
    "    tfidf_vectorizer.fit_transform(train[text_field].values)\n",
    "    train_vectorized = tfidf_vectorizer.transform(train[text_field].values)\n",
    "    test_vectorized = tfidf_vectorizer.transform(test[text_field].values)\n",
    "    return train_vectorized, test_vectorized, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    985\n",
       "1    530\n",
       "2    208\n",
       "3    360\n",
       "4    318\n",
       "5    588\n",
       "6    249\n",
       "Name: url, dtype: int64"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby(\"General Typing\")[[\"url\"]].count().reset_index()[\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsmote = SMOTE(\\n    sampling_strategy={k: max(int(max(dataset_counts) * 0.6), int(dataset_counts.iloc[k] * 1.4)) for k in range(7)} #\\n)\\nX_train, Y_train = smote.fit_resample(X_train, Y_train)\\n'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dataset = pd.DataFrame()\n",
    "dataset_counts = dataset.groupby(\"General Typing\")[[\"url\"]].count().reset_index()[\"url\"]\n",
    "license_dict = {\n",
    "        \"by\": 0,\n",
    "        \"by-nc\": 1,\n",
    "        \"by-nc-nd\": 2,\n",
    "        \"by-nc-sa\": 3,\n",
    "        \"by-nd\": 4,\n",
    "        \"by-sa\": 5,\n",
    "        \"publicdomain\": 6\n",
    "}\n",
    "\n",
    "model_dataset[\"train_text\"], model_dataset[\"General Typing\"] = dataset[\"cleaned_contents\"], dataset[\"General Typing\"]\n",
    "model_dataset[\"General Typing\"].replace(\n",
    "    license_dict,\n",
    "    inplace = True\n",
    ")\n",
    "\n",
    "training_set, test_set = train_test_split(model_dataset, test_size = 0.5)\n",
    "Y_train = training_set[\"General Typing\"].values\n",
    "Y_test = test_set[\"General Typing\"].values\n",
    "X_train, X_test, model_vecter = extract_text_features_tfidf(\n",
    "    training_set, test_set\n",
    ")\n",
    "'''\n",
    "smote = SMOTE(\n",
    "    sampling_strategy={k: max(int(max(dataset_counts) * 0.6), int(dataset_counts.iloc[k] * 1.4)) for k in range(7)} #\n",
    ")\n",
    "X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logreg = LogisticRegression(\n",
    "    verbose = 0,\n",
    "    max_iter = 1000,\n",
    "    penalty = 'l2',\n",
    "    solver = \"liblinear\",\n",
    "    C = 5\n",
    ").fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9252625077208153 \n",
      " 0.9746757257566399 \n",
      " 0.9870290302655961 \n",
      "==============\n",
      " 0.38604076590487957 \n",
      " 0.5663990117356393 \n",
      " 0.7022853613341569\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    top_k_accuracy_score(Y_train, model_logreg.predict_proba(X_train), k = 1),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_train, model_logreg.predict_proba(X_train), k = 2),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_train, model_logreg.predict_proba(X_train), k = 3),\n",
    "    \"\\n==============\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_logreg.predict_proba(X_test), k = 1),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_logreg.predict_proba(X_test), k = 2),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_logreg.predict_proba(X_test), k = 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "model_svc = SVC(\n",
    "    C = 5,\n",
    "    verbose = 1,\n",
    "    probability = True\n",
    ").fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9376158122297714 \n",
      " 0.9765287214329833 \n",
      " 0.9888820259419395 \n",
      "==============\n",
      " 0.40271772699197034 \n",
      " 0.58307597282273 \n",
      " 0.6991970352069179\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    top_k_accuracy_score(Y_train, model_svc.predict_proba(X_train), k = 1),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_train, model_svc.predict_proba(X_train), k = 2),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_train, model_svc.predict_proba(X_train), k = 3),\n",
    "    \"\\n==============\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_svc.predict_proba(X_test), k = 1),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_svc.predict_proba(X_test), k = 2),\n",
    "    \"\\n\",\n",
    "    top_k_accuracy_score(Y_test, model_svc.predict_proba(X_test), k = 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2'\n",
    "tfhub_handle_encoder = 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "TRAINING_RATIO = 0.7\n",
    "\n",
    "model_dataset_copy = dataset.loc[:, [\"cleaned_contents\", \"General Typing\"]]\n",
    "model_dataset_copy[\"General Typing\"].replace(\n",
    "    license_dict,\n",
    "    inplace = True\n",
    ")\n",
    "target = model_dataset_copy.pop(\"General Typing\")\n",
    "# dataset_tensor = tf.convert_to_tensor(model_dataset, dtype=np.string_)\n",
    "tf_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((model_dataset_copy, target))\n",
    "    .shuffle(50)\n",
    ")\n",
    "train_dataset = tf_dataset.take(int(TRAINING_RATIO * len(tf_dataset)))\n",
    "test_dataset = tf_dataset.skip(int(TRAINING_RATIO * len(tf_dataset)))\n",
    "val_dataset = test_dataset.skip(\n",
    "    int((1 - TRAINING_RATIO) * 0.5 * len(tf_dataset))\n",
    ")\n",
    "test_dataset = test_dataset.take(\n",
    "    int((1 - TRAINING_RATIO) * 0.5 * len(tf_dataset))\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='parsed_cleaned_contents')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.5)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 1e-7\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "31/31 [==============================] - 110s 3s/step - loss: 2.7991e-07 - accuracy: 0.2700 - val_loss: 4.2688e-07 - val_accuracy: 0.0429\n",
      "Epoch 2/10\n",
      "31/31 [==============================] - 109s 4s/step - loss: 2.7697e-07 - accuracy: 0.2423 - val_loss: 4.3029e-07 - val_accuracy: 0.0286\n",
      "Epoch 3/10\n",
      "17/31 [===============>..............] - ETA: 45s - loss: 1.8846e-07 - accuracy: 0.3033"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\brand\\Desktop\\UCBF22\\DSD Fall 2022\\model_sampling\\model_dev.ipynb Cell 38\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m classifier_model \u001b[39m=\u001b[39m build_classifier_model()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m classifier_model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m history \u001b[39m=\u001b[39m classifier_model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train_dataset,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brand/Desktop/UCBF22/DSD%20Fall%202022/model_sampling/model_dev.ipynb#X52sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1385\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2957\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1854\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "classifier_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = classifier_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd98d7e735064cd82a69955e27f6d0d35e8c81af503a9f702e870f7c3b7bf96a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
